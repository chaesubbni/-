{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcbd176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BeautifulSoup4\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from BeautifulSoup4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from BeautifulSoup4)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: typing-extensions, soupsieve, BeautifulSoup4\n",
      "\n",
      "   ---------------------------------------- 3/3 [BeautifulSoup4]\n",
      "\n",
      "Successfully installed BeautifulSoup4-4.14.2 soupsieve-2.8 typing-extensions-4.15.0\n"
     ]
    }
   ],
   "source": [
    "! pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9448527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734078de",
   "metadata": {},
   "source": [
    "## 연습용 HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07678cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬에서 직접 HTML 문자열 작성\n",
    "html = '''\n",
    "<h1 id=\"title\">한빛출판네트워크</h1>\n",
    "<div class=\"top\">\n",
    "    <ul class=\"menu\">\n",
    "        <li><a href=\"http://www.hanbit.co.kr/member/login.html\" class=\"login\">로그인</a></li>\n",
    "    </ul>\n",
    "\n",
    "    <ul class=\"brand\">\n",
    "        <li><a href=\"http://www.hanbit.co.kr/media/\">한빛미디어</a></li>\n",
    "        <li><a href=\"http://www.hanbit.co.kr/academy/\">한빛아카데미</a></li>\n",
    "    </ul>\n",
    "</div>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2792ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup 객체 생성\n",
    "soup = BeautifulSoup(html, \"html.parser\") # 파서는 그냥 html을 파이썬에선 문쟈열로 인식하는데\n",
    "# 그걸 계층 구조로 바꿔 html 자체를 인식시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee1fb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목: 한빛출판네트워크\n"
     ]
    }
   ],
   "source": [
    "# 1. h1 태그 선택\n",
    "title = soup.find(\"h1\").get_text()\n",
    "#title = soup.find(\"h1\").get_text()\n",
    "print(\"제목:\", title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d7d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로그인 텍스트: 로그인\n",
      "로그인 링크: http://www.hanbit.co.kr/member/login.html\n"
     ]
    }
   ],
   "source": [
    "# 2. 로그인 링크(a 태그) 가져오기\n",
    "login_tag = soup.find(\"a\", class_ = \"login\")\n",
    "print(\"로그인 텍스트:\", login_tag.get_text())\n",
    "print(\"로그인 링크:\", login_tag[\"href\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b13ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "브랜드 리스트:\n",
      "- 한빛미디어 → http://www.hanbit.co.kr/media/\n",
      "- 한빛아카데미 → http://www.hanbit.co.kr/academy/\n"
     ]
    }
   ],
   "source": [
    "# 3. 브랜드 목록 가져오기\n",
    "brand_list = soup.find(\"ul\", class_=\"brand\").find_all(\"a\")\n",
    "print(\"\\n브랜드 리스트:\")\n",
    "for b in brand_list:\n",
    "    print(\"-\", b.get_text(), \"→\", b[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc400c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li><a href=\"http://www.hanbit.co.kr/media/\">한빛미디어</a></li>, <li><a href=\"http://www.hanbit.co.kr/academy/\">한빛아카데미</a></li>]\n"
     ]
    }
   ],
   "source": [
    "brand_list = soup.find(\"ul\", class_ = \"brand\").find_all(\"li\")\n",
    "print(brand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "015b9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조금 더 난이도 있는 html\n",
    "# 태그/클래스/id/속성/형제/부모/자식 요소 모두 연습 가능하도록 구성됨\n",
    "html = '''\n",
    "<div id=\"container\">\n",
    "\n",
    "    <h1 id=\"main-title\">에이콘의 크롤링 연습</h1>\n",
    "\n",
    "    <p class=\"desc\">이 페이지는 BeautifulSoup를 연습하기 위한 예제입니다.</p>\n",
    "\n",
    "    <div class=\"user-info\">\n",
    "        <span class=\"label\">작성자:</span>\n",
    "        <span class=\"value\">Acon</span>\n",
    "    </div>\n",
    "\n",
    "    <ul class=\"menu\">\n",
    "        <li><a href=\"/home\" class=\"nav home\">홈</a></li>\n",
    "        <li><a href=\"/about\" class=\"nav about\">소개</a></li>\n",
    "        <li><a href=\"/contact\" class=\"nav contact\">문의</a></li>\n",
    "    </ul>\n",
    "\n",
    "    <div class=\"products\">\n",
    "        <h2>상품 목록</h2>\n",
    "        <ul>\n",
    "            <li data-id=\"101\" class=\"item\">\n",
    "                <span class=\"name\">크롤링 중급</span>\n",
    "                <span class=\"price\">390000</span>\n",
    "            </li>\n",
    "            <li data-id=\"102\" class=\"item special\">\n",
    "                <span class=\"name\">이미지 크롤링</span>\n",
    "                <span class=\"price\">2700000</span>\n",
    "            </li>\n",
    "            <li data-id=\"103\" class=\"item\">\n",
    "                <span class=\"name\">파이썬 성장 클래스</span>\n",
    "                <span class=\"price\">5900000</span>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"footer\">\n",
    "        <p>© 2025 LalaMom Corp.</p>\n",
    "        <a href=\"https://instagram.com/Acon\" class=\"sns ig\">인스타그램</a>\n",
    "        <a href=\"https://youtube.com/Acon\" class=\"sns yt\">유튜브</a>\n",
    "    </div>\n",
    "\n",
    "</div>\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49633a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\") # html은 파이썬이 문자열로 인식하니깐 그걸 트리 계층으로 구분시킴.-> html.parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5104588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] h1 제목: 에이콘의 크롤링 연습\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 1. find() → 가장 첫 번째 요소 /조건에 맞는 첫 번째 요소 하나만 찾아줌\n",
    "title = soup.find(\"h1\").get_text()\n",
    "print(\"[1] h1 제목:\", title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ded2a829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n",
      "/about\n",
      "/contact\n",
      "https://instagram.com/Acon\n",
      "https://youtube.com/Acon\n"
     ]
    }
   ],
   "source": [
    "all_links = soup.find_all(\"a\")\n",
    "for i in all_links:\n",
    "    print(i[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caf5c6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] 링크 개수: 5\n",
      " - 홈 → /home\n",
      " - 소개 → /about\n",
      " - 문의 → /contact\n",
      " - 인스타그램 → https://instagram.com/Acon\n",
      " - 유튜브 → https://youtube.com/Acon\n"
     ]
    }
   ],
   "source": [
    "# 2. find_all() → 모든 a 태그 /조건에 맞는 태그를 모두 리스트 형태로 가져옴\n",
    "all_links = soup.find_all(\"a\")\n",
    "print(\"[2] 링크 개수:\", len(all_links))\n",
    "for link in all_links:\n",
    "    print(\" -\", link.get_text(), \"→\", link[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89ba1dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] 설명: 이 페이지는 BeautifulSoup를 연습하기 위한 예제입니다.\n"
     ]
    }
   ],
   "source": [
    "# 3. class로 요소 찾기\n",
    "# <p class=\"desc\"> ... </p> 형태의 태그를 찾는 것\n",
    "# class라는 단어는 파이썬 예약어라 class_= 로 씀\n",
    "desc = soup.find(\"p\", class_=\"desc\").get_text()\n",
    "print(\"[3] 설명:\", desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6543809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] 메인 타이틀: 에이콘의 크롤링 연습\n"
     ]
    }
   ],
   "source": [
    "# 4. id로 특정 요소 찾기\n",
    "# id는 HTML 문서 내에서 “유일(unique)한 값”\n",
    "# 그래서 find(id=\"값\") 하면 거의 정확하게 하나만 찾아짐\n",
    "main_title = soup.find(id=\"main-title\").get_text() # id로 찾을땐 태그 안 적어도 됨.\n",
    "# 왜냐면 id는 고유번호임.\n",
    "print(\"[4] 메인 타이틀:\", main_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da818e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] 상품 정보:\n",
      " - [101] 크롤링 중급 : 390000원\n",
      " - [102] 이미지 크롤링 : 2700000원\n",
      " - [103] 파이썬 성장 클래스 : 5900000원\n"
     ]
    }
   ],
   "source": [
    "# 5. 속성 접근(data-id, href 등)\n",
    "# <li class=\"item\" data-id=\"101\"> 같은 상품 li 태그 전부 찾기\n",
    "# li : List Item / 리스트에서 한 줄 한 줄을 만드는 태그\n",
    "product_items = soup.find_all(\"li\", class_=\"item\")\n",
    "print(\"[5] 상품 정보:\")\n",
    "for item in product_items:\n",
    "    item_id = item[\"data-id\"] \n",
    "    name = item.find(\"span\", class_=\"name\").get_text()\n",
    "    price = item.find(\"span\", class_=\"price\").get_text()\n",
    "    print(f\" - [{item_id}] {name} : {price}원\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72db81b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 크롤링 중급\n",
      "102 이미지 크롤링\n",
      "103 파이썬 성장 클래스\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "product_items = soup.find_all(\"li\", class_ = \"item\") # class 명에 item이 포함되어 있으면 추출됨.\n",
    "# 근데 item abc 이렇게 띄어쓰기까진 가능인데 items나 itemabc나 다른 문자열이 붙어 있으면 추출 안됨.\n",
    "for i in product_items:\n",
    "    print(i[\"data-id\"], i.find(\"span\", class_ = \"name\").get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae621b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'103'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 태그의 속성(data-id) 값을 꺼내기\n",
    "# <li> 태그 안의 data-id 속성 값만 가져오기 \n",
    "# 태그의 속성을 꺼내려면 딕셔너리처럼 접근\n",
    "item[\"data-id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd03ee25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"name\">파이썬 성장 클래스</span>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.find(\"span\", class_=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db856647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] 메뉴 링크:\n",
      " * 홈 → /home\n",
      " * 소개 → /about\n",
      " * 문의 → /contact\n"
     ]
    }
   ],
   "source": [
    "# 6. select() → CSS 선택자로 검색\n",
    "# .menu 클래스 아래의 li 안의 a\n",
    "# CSS 문법 그대로 적용됨\n",
    "# find보다 조금 더 유연하고 직관적이라 실무에서 더 많이 씀\n",
    "\n",
    "menu_links = soup.select(\".menu li a\") # .menu는 클래스가 menu인 얘들 찾고 그 안에 li 안에 a 찾음.\n",
    "print(\"[6] 메뉴 링크:\")\n",
    "for m in menu_links:\n",
    "    print(\" *\", m.get_text(), \"→\", m[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a42449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"nav home\" href=\"/home\">홈</a>, <a class=\"nav about\" href=\"/about\">소개</a>, <a class=\"nav contact\" href=\"/contact\">문의</a>]\n"
     ]
    }
   ],
   "source": [
    "menu_links = soup.select(\".menu li a\") # .을 붙이면 class, #을 붙이면 id\n",
    "print(menu_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e227d06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] 인스타 링크: https://instagram.com/Acon\n",
      "<a class=\"sns ig\" href=\"https://instagram.com/Acon\">인스타그램</a>\n"
     ]
    }
   ],
   "source": [
    "# 7. select_one()\n",
    "#.sns & .ig 클래스를 모두 가진 태그 하나 선택\n",
    "# \"한 개만” 찾고 싶을 때 사용\n",
    "ig_link = soup.select_one(\".sns.ig\") # class가 sns도 포함되고 ig인 얘들 찾음.\n",
    "# 즉, sns도 들어가고 ig도 들어간 클래스를 찾음.\n",
    "print(\"[7] 인스타 링크:\", ig_link[\"href\"])\n",
    "\n",
    "ig_link = soup.select_one(\".sns.ig\")\n",
    "print(ig_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "271013b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] '상품 name span'의 parent 태그: li\n"
     ]
    }
   ],
   "source": [
    "# 8. parent 찾기 / 부모 태그 접근\n",
    "# <span class=\"name\"> 요소의 부모는 <li>\n",
    "# → “span이 포함된 li는 어떤 태그인지” 확인할 때 유용\n",
    "product_section = soup.find(\"span\", class_=\"name\").parent\n",
    "print(\"[8] '상품 name span'의 parent 태그:\", product_section.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8df8264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] footer children 개수: 7\n",
      "\n",
      "© 2025 LalaMom Corp.\n",
      "\n",
      "인스타그램\n",
      "\n",
      "유튜브\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. children / contents -> 자식 요소들 리스트\n",
    "# footer 안에 있는 모든 하위 요소(tag, 공백 포함)를 리스트로 변환\n",
    "# 레이아웃 구조 확인할 때 좋음\n",
    "footer_children = list(soup.find(\"div\", class_=\"footer\").children)\n",
    "print(\"[9] footer children 개수:\", len(footer_children)) \n",
    "#공백까지 포함해서 7개\n",
    "for child in footer_children:\n",
    "    if hasattr(child, 'get_text'): # get_text라는 함수를 가지고 있는지 확인하기.\n",
    "        print(child.get_text(strip=True)) # 양끝 공백 삭제."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0681e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] special 다음 상품: 파이썬 성장 클래스\n"
     ]
    }
   ],
   "source": [
    "# 10. next_sibling / previous_sibling\n",
    "# HTML은 태그 사이에 “줄바꿈/공백”도 sibling으로 인식함\n",
    "# → 그래서 두 번 이동해야 실제 다음 li 태그에 도달\n",
    "special_item = soup.find(\"li\", class_=\"special\")\n",
    "next_item = special_item.next_sibling.next_sibling  # 공백 노드 고려\n",
    "print(\"[10] special 다음 상품:\", next_item.find('span', class_='name').get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3f32e",
   "metadata": {},
   "source": [
    "## 네이버 뉴스 페이지 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f61adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. “네이버 뉴스처럼 생긴” 연습용 HTML\n",
    "html = '''\n",
    "<!doctype html>\n",
    "<html lang=\"ko\">\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>연습용 네이버 뉴스</title>\n",
    "</head>\n",
    "<body>\n",
    "<div id=\"contents\">\n",
    "\n",
    "    <div class=\"media_end_head\">\n",
    "        <h2 class=\"media_end_head_headline\">\n",
    "            정부, 2025년 공공데이터 고도화 계획 발표…\"AI 활용성 강화\"\n",
    "        </h2>\n",
    "\n",
    "        <div class=\"media_end_head_info\">\n",
    "            <a href=\"#\" class=\"press media_end_head_top_logo\">뉴시스</a>\n",
    "            <span class=\"journalist\">\n",
    "                <span class=\"media_end_head_journalist_name\">라라 기자</span>\n",
    "            </span>\n",
    "            <span class=\"media_end_head_info_datestamp_time\">2025.11.25. 오전 09:30</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div id=\"newsct_article\">\n",
    "        <div id=\"dic_area\">\n",
    "            <p>정부는 2025년 공공데이터 개방 정책을 강화하고, AI 활용도를 높이기 위한 새로운 계획을 발표했다.</p>\n",
    "            <p>이번 개편안에는 데이터 품질 개선, 민간 협업 확대, API 접근성 향상 등이 포함된다.</p>\n",
    "            <p>전문가들은 \"공공데이터의 실질적 활용이 확대될 것\"이라고 전망하고 있다.</p>\n",
    "        </div>\n",
    "\n",
    "        <ul class=\"media_end_categorize\">\n",
    "            <li class=\"media_end_categorize_item\">#공공데이터</li>\n",
    "            <li class=\"media_end_categorize_item\">#AI</li>\n",
    "            <li class=\"media_end_categorize_item\">#정부정책</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"media_end_linked\">\n",
    "        <h3>관련 기사</h3>\n",
    "        <ul>\n",
    "            <li><a href=\"/article1\">\"AI 산업에 미치는 영향 분석\"</a></li>\n",
    "            <li><a href=\"/article2\">데이터 개방 확대가 불러올 변화</a></li>\n",
    "        </ul>\n",
    "    </div>\n",
    "\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46f1965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<meta charset=\"utf-8\"/>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "title = soup.find(\"meta\", charset=\"utf-8\")\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "132cd61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴시스\n",
      "뉴시스\n"
     ]
    }
   ],
   "source": [
    "title = soup.select_one(\"a.press.media_end_head_top_logo\").get_text()\n",
    "print(title)\n",
    "\n",
    "title = soup.find(\"a\", class_=[\"press\", \"media_end_head_top_logo\"])\n",
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e7efa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라라 기자\n"
     ]
    }
   ],
   "source": [
    "article = title.next_sibling.next_sibling\n",
    "print(list(article)[1].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5370d",
   "metadata": {},
   "source": [
    "- 네이버 뉴스는 대부분 div 아이디 기반으로 구조화됨\n",
    "- 제목, 기자, 날짜 등은 class 기반으로 잘 구분되어 있음\n",
    "- 본문은 보통 div#dic_area 내부에 존재\n",
    "- 키워드는 li 리스트로 구성됨\n",
    "- 관련기사도 ul/li 구조로 이루어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4b424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 기사 제목: 정부, 2025년 공공데이터 고도화 계획 발표…\"AI 활용성 강화\"\n",
      "[2] 언론사: 뉴시스\n",
      "[3] 기자: 라라 기자\n",
      "[4] 날짜: 2025.11.25. 오전 09:30\n",
      "\n",
      "[5] 본문 내용:\n",
      "- 정부는 2025년 공공데이터 개방 정책을 강화하고, AI 활용도를 높이기 위한 새로운 계획을 발표했다.\n",
      "- 이번 개편안에는 데이터 품질 개선, 민간 협업 확대, API 접근성 향상 등이 포함된다.\n",
      "- 전문가들은 \"공공데이터의 실질적 활용이 확대될 것\"이라고 전망하고 있다.\n",
      "\n",
      "[6] 키워드 목록: ['#공공데이터', '#AI', '#정부정책']\n",
      "\n",
      "[7] 관련 기사:\n",
      "- \"AI 산업에 미치는 영향 분석\" → /article1\n",
      "- 데이터 개방 확대가 불러올 변화 → /article2\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 1. 기사 제목\n",
    "# find로 특정 태그를 하나 찾고, class로 정확히 골라서 get_text로 글자만 뽑아오는 기능\n",
    "title = soup.find(\"h2\", class_=\"media_end_head_headline\").get_text(strip=True)\n",
    "print(\"[1] 기사 제목:\", title)\n",
    "\n",
    "# 2. 언론사\n",
    "press = soup.find(\"a\", class_=\"press\").get_text(strip=True)\n",
    "print(\"[2] 언론사:\", press)\n",
    "\n",
    "# 3. 기자 이름\n",
    "author = soup.find(\"span\", class_=\"media_end_head_journalist_name\").get_text(strip=True)\n",
    "print(\"[3] 기자:\", author)\n",
    "\n",
    "# 4. 날짜\n",
    "date = soup.find(\"span\", class_=\"media_end_head_info_datestamp_time\").get_text(strip=True)\n",
    "print(\"[4] 날짜:\", date)\n",
    "\n",
    "# 5. 본문 전체\n",
    "article = soup.find(\"div\", id=\"dic_area\")\n",
    "paragraphs = article.find_all(\"p\")\n",
    "print(\"\\n[5] 본문 내용:\")\n",
    "for p in paragraphs:\n",
    "    print(\"-\", p.get_text(strip=True))\n",
    "\n",
    "# 6. 키워드\n",
    "keywords = soup.select(\".media_end_categorize_item\")\n",
    "keyword_list = [k.get_text(strip=True) for k in keywords]\n",
    "print(\"\\n[6] 키워드 목록:\", keyword_list)\n",
    "\n",
    "# 7. 관련 기사\n",
    "related = soup.select(\".media_end_linked ul li a\")\n",
    "print(\"\\n[7] 관련 기사:\")\n",
    "for r in related:\n",
    "    print(\"-\", r.get_text(strip=True), \"→\", r[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d403469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "정부는 2025년 공공데이터 개방 정책을 강화하고, AI 활용도를 높이기 위한 새로운 계획을 발표했다.\n",
      "\n",
      "이번 개편안에는 데이터 품질 개선, 민간 협업 확대, API 접근성 향상 등이 포함된다.\n",
      "\n",
      "전문가들은 \"공공데이터의 실질적 활용이 확대될 것\"이라고 전망하고 있다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content = soup.select_one(\"div#dic_area\").children # select는 항상 리스트를 반환함.\n",
    "for i in content:\n",
    "    if hasattr(i, \"get_text\"):\n",
    "        print(i.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd2b02a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정부는 2025년 공공데이터 개방 정책을 강화하고, AI 활용도를 높이기 위한 새로운 계획을 발표했다.\n",
      "이번 개편안에는 데이터 품질 개선, 민간 협업 확대, API 접근성 향상 등이 포함된다.\n",
      "전문가들은 \"공공데이터의 실질적 활용이 확대될 것\"이라고 전망하고 있다.\n"
     ]
    }
   ],
   "source": [
    "content = soup.find(\"div\", id=\"dic_area\")\n",
    "content = content.find_all(\"p\")\n",
    "\n",
    "for i in content:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000936df",
   "metadata": {},
   "source": [
    "## hollys 페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6ce8c",
   "metadata": {},
   "source": [
    "- 내가 크롤링 할 사이트 맨 뒤에 /robots.txt를 입력하면 크롤링 유무 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a49c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CODE 1]\n",
    "def hollys_store(result):\n",
    "    for page in range(1,47):\n",
    "        Hollys_url = 'https://www.hollys.co.kr/store/korea/korStore2.do?pageNo={}&sido=&gugun=&store='.format(page)\n",
    "        print(Hollys_url)\n",
    "        html = urllib.request.urlopen(Hollys_url)\n",
    "        if html.getcode() != 200:\n",
    "            print(\"비상비상\")\n",
    "        soupHollys = BeautifulSoup(html, 'html.parser')\n",
    "        tag_tbody = soupHollys.find('tbody')\n",
    "        for store in tag_tbody.find_all('tr'):\n",
    "            if len(store) <= 3:\n",
    "                break\n",
    "            store_td = store.find_all('td')\n",
    "            store_name = store_td[1].string\n",
    "            store_sido = store_td[0].string\n",
    "            store_address = store_td[3].string\n",
    "            store_phone = store_td[5].string\n",
    "            result.append([store_name]+[store_sido]+[store_address]\n",
    "                          +[store_phone])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e2af2cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hollys store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=1&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=2&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=3&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=4&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=5&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=6&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=7&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=8&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=9&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=10&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=11&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=12&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=13&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=14&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=15&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=16&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=17&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=18&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=19&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=20&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=21&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=22&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=23&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=24&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=25&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=26&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=27&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=28&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=29&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=30&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=31&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=32&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=33&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=34&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=35&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=36&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=37&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=38&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=39&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=40&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=41&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=42&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=43&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=44&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=45&sido=&gugun=&store=\n",
      "https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=46&sido=&gugun=&store=\n",
      "hollys1.csv  파일저장 완료 >>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "#[CODE 0]\n",
    "def main():\n",
    "    result = []\n",
    "    print('Hollys store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    hollys_store(result)   #[CODE 1] 호출 \n",
    "    hollys_tbl = pd.DataFrame(result, columns=('store', 'sido-gu', 'address','phone'))\n",
    "    hollys_tbl.to_csv('hollys1.csv', encoding='utf8', mode='w', index=True)\n",
    "    print('hollys1.csv  파일저장 완료 >>>>>>>>>>>>>>>>>')\n",
    "       \n",
    "if __name__ == '__main__':\n",
    "     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3297412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
